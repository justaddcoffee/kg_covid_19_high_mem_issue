{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High mem issue\n",
    "This nb is to reproduce an issue where generating negative edges consumes a large\n",
    "amount of memory and requires a long time\n",
    "\n",
    "Graphs and embeddings used in the NB are available here:\n",
    "http://doi.org/10.5281/zenodo.4011267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f687261d8119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msilence_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;31m# Import needed to avoid TensorFlow warnings and general useless infos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/kg_covid_19_high_mem_issue/venv/lib/python3.7/site-packages/silence_tensorflow/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msilence_tensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msilence_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msilence_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/kg_covid_19_high_mem_issue/venv/lib/python3.7/site-packages/silence_tensorflow/silence_tensorflow.py\u001b[0m in \u001b[0;36msilence_tensorflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KMP_AFFINITY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"noverbose\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF_CPP_MIN_LOG_LEVEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graph and embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a simple Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import AUC, Recall, Precision\n",
    "\n",
    "def build_link_prediction_model(input_shape:int):\n",
    "    model = Sequential([\n",
    "        Input(input_shape),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\n",
    "            AUC(curve=\"PR\", name=\"auprc\"),\n",
    "            AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "            Recall(),\n",
    "            Precision(),\n",
    "            \"accuracy\"\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "graph = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"merged-kg_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_edge_type=\"biolink:association\",\n",
    "    node_path=\"/home/jtr4v/merged-kg_nodes.tsv\",\n",
    "    nodes_column=\"id\",\n",
    "    node_types_column=\"category\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining holdouts and tasks data generator\n",
    "We are going to create the same edge embeddings as in the training of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train_percentage = 0.8\n",
    "\n",
    "pos_training, pos_validation = graph.connected_holdout(seed, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python /home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/run.py edges -n /home/jtr4v/merged-kg_nodes-min.tsv -e /home/jtr4v/merged-kg_edges-min.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_training, neg_validation = graph.sample_negatives(\n",
    "#    seed=seed,\n",
    "#    negatives_number=graph.get_edges_number(),\n",
    "#    allow_selfloops=False\n",
    "#).random_holdout(seed=seed, train_percentage=train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "neg_validation = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"/home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/data/edges/neg_test_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")\n",
    "\n",
    "neg_training = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"/home/jtr4v/kg_covid_19_drug_analyses/kg-covid-19/data/edges/neg_train_edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    edge_types_column=\"edge_label\",\n",
    "    default_node_type=\"biolink:NamedThing\",\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from embiggen import GraphTransformer, EdgeTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def task_generator(\n",
    "    pos_training:EnsmallenGraph,\n",
    "    pos_validation:EnsmallenGraph,\n",
    "    neg_training:EnsmallenGraph,\n",
    "    neg_validation:EnsmallenGraph,\n",
    "    train_percentage:float=train_percentage,\n",
    "    seed:int=seed\n",
    "):\n",
    "    \"\"\"Create new generator of tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------\n",
    "    pos_training:EnsmallenGraph,\n",
    "        The positive edges of the training graph.\n",
    "    pos_validation:EnsmallenGraph,\n",
    "        The positive edges of the validation graph.\n",
    "    neg_training:EnsmallenGraph,\n",
    "        The negative edges of the training graph.\n",
    "    neg_validation:EnsmallenGraph,\n",
    "        The negative edges of the validation graph.\n",
    "    train_percentage:float=0.8,\n",
    "    seed:int=42\n",
    "\n",
    "    \"\"\"\n",
    "    for path in tqdm(glob(\"*embedding.npy\"), desc=\"Embedding\"):\n",
    "        model_name = path.split(\"_\")[0]\n",
    "        embedding = np.load(path)\n",
    "        for method in tqdm(EdgeTransformer.methods, desc=\"Methods\", leave=False):\n",
    "            transformer = GraphTransformer(method)\n",
    "            transformer.fit(embedding)\n",
    "            train_edges = np.vstack([\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_training, neg_training)\n",
    "            ])\n",
    "            valid_edges = np.vstack([\n",
    "                transformer.transform(graph)\n",
    "                for graph in (pos_validation, neg_validation)\n",
    "            ])\n",
    "            train_labels = np.concatenate([\n",
    "                np.ones(pos_training.get_edges_number()),\n",
    "                np.zeros(neg_training.get_edges_number())\n",
    "            ])\n",
    "            valid_labels = np.concatenate([\n",
    "                np.ones(pos_validation.get_edges_number()),\n",
    "                np.zeros(neg_validation.get_edges_number())\n",
    "            ])\n",
    "            train_indices = np.arange(0, train_labels.size)\n",
    "            valid_indices = np.arange(0, valid_labels.size)\n",
    "            np.random.shuffle(train_indices)\n",
    "            np.random.shuffle(valid_indices)\n",
    "            train_edges = train_edges[train_indices]\n",
    "            train_labels = train_labels[train_indices]\n",
    "            valid_edges = valid_edges[valid_indices]\n",
    "            valid_labels = valid_labels[valid_indices]\n",
    "            yield model_name, method, (train_edges, train_labels), (valid_edges, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "histories = {}\n",
    "strategy = MirroredStrategy()\n",
    "os.makedirs(\"classical_link_prediction\", exist_ok=True)\n",
    "\n",
    "for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "    history_path = f\"classical_link_prediction/{embedding_model}_{method}.csv\"\n",
    "    if os.path.exists(history_path):\n",
    "        histories[(embedding_model, method)] = pd.read_csv(history_path)\n",
    "        continue\n",
    "    with strategy.scope():\n",
    "        model = build_link_prediction_model(train[0].shape[1:])\n",
    "    history = pd.DataFrame(model.fit(\n",
    "        *train,\n",
    "        batch_size=2**12,\n",
    "        validation_data=valid,\n",
    "        epochs=1000,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\"val_loss\", patience=20, min_delta=0.0001),\n",
    "            ReduceLROnPlateau()\n",
    "        ]\n",
    "    ).history)\n",
    "\n",
    "    history.to_csv(history_path, index=False)\n",
    "    histories[(embedding_model, method)] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all the computer histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "\n",
    "for history in histories.values():\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of various embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we covert the histories into an homogeneous report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sanitize_ml_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5b83b019dd2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msanitize_ml_labels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_ml_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlast_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sanitize_ml_labels'"
     ]
    }
   ],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "\n",
    "report = []\n",
    "for (model, method), history in histories.items():\n",
    "    last_epoch = history.iloc[-1].to_dict()\n",
    "    sanitize = {\n",
    "        sanitize_ml_labels(label):value\n",
    "        for label, value in last_epoch.items()\n",
    "        if label not in (\"lr\")\n",
    "    }\n",
    "    training = {\n",
    "        key:val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" not in key\n",
    "    }\n",
    "    validation = {\n",
    "        sanitize_ml_labels(key.replace(\"Val \", \"\")):val\n",
    "        for key, val in sanitize.items()\n",
    "        if \"Val\" in key\n",
    "    }\n",
    "\n",
    "    report.append({\n",
    "        \"run\":\"training\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **training\n",
    "    })\n",
    "    report.append({\n",
    "        \"run\":\"validation\",\n",
    "        \"embedding_model\":model,\n",
    "        \"model\":\"MLP\",\n",
    "        \"method\":method,\n",
    "        **validation\n",
    "    })\n",
    "\n",
    "report = pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training link prediction on some other models\n",
    "Here we execute the link prediction using Random Forests, Decision Trees and Logistic Regression so to have a good comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sanitize_ml_labels import sanitize_ml_labels\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "kwargs = {\n",
    "    \"DecisionTreeClassifier\":dict(\n",
    "        max_depth=30,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"RandomForestClassifier\":dict(\n",
    "        n_estimators=500,\n",
    "        max_depth=30,\n",
    "        n_jobs=cpu_count(),\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LogisticRegression\":dict(\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "def metric_report(y_true, y_pred):\n",
    "    metrics = (\n",
    "        roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "    )\n",
    "    return {\n",
    "        sanitize_ml_labels(metric.__name__):metric(y_true, y_pred)\n",
    "        for metric in metrics\n",
    "    }\n",
    "\n",
    "metrics_reports_path = \"classical_link_prediction/linear_models_reports.csv\"\n",
    "if os.path.exists(metrics_reports_path):\n",
    "    metrics_reports = pd.read_csv(metrics_reports_path)\n",
    "else:\n",
    "    metrics_reports = []\n",
    "\n",
    "    for embedding_model, method, train, valid in task_generator(pos_training, pos_validation, neg_training, neg_validation):\n",
    "        for model_builder in tqdm((DecisionTreeClassifier, RandomForestClassifier, LogisticRegression), desc=\"Model\", leave=False):\n",
    "            model = model_builder(**kwargs[model_builder.__name__])\n",
    "            train_x, train_y = train\n",
    "            valid_x, valid_y = valid\n",
    "            model.fit(train_x, train_y)\n",
    "            train_y_pred = model.predict(train_x)\n",
    "            valid_y_pred = model.predict(valid_x)\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"training\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(train_y, train_y_pred)\n",
    "            })\n",
    "            metrics_reports.append({\n",
    "                \"run\":\"validation\",\n",
    "                \"embedding_model\":embedding_model,\n",
    "                \"model\":model_builder.__name__,\n",
    "                \"method\":method,\n",
    "                **metric_report(valid_y, valid_y_pred)\n",
    "            })\n",
    "\n",
    "    metrics_reports = pd.DataFrame(metrics_reports)\n",
    "    metrics_reports.to_csv(metrics_reports_path, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = pd.concat([\n",
    "    metrics_reports,\n",
    "    report\n",
    "])\n",
    "\n",
    "all_reports.to_csv(\"classical_link_prediction/all_reports.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from barplots import barplots\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# show_standard_deviation is False because there is only one holdout!\n",
    "barplots(\n",
    "    all_reports,\n",
    "    [\"run\", \"method\", \"embedding_model\", \"model\"],\n",
    "    path = 'barplots/{feature}.jpg',\n",
    "    show_standard_deviation=False,\n",
    "    height=5,\n",
    "    subplots=True,\n",
    "    plots_per_row=1\n",
    ")\n",
    "\n",
    "for barplot_path in glob(\"barplots/*\"):\n",
    "    display(Image.open(barplot_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "scored_per_method = [\n",
    "    (group, x[\"AUPRC\"].values)\n",
    "    for group, x in list(all_reports[[\"AUPRC\", \"method\"]].groupby(\"method\"))\n",
    "]\n",
    "\n",
    "for i, (method1, scores1) in enumerate(scored_per_method):\n",
    "    for method2, scores2 in scored_per_method[i+1:]:\n",
    "        print(\n",
    "            method1, method2, wilcoxon(scores1, scores2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
